{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary modules\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "\n",
        "# Downloading required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q9IzVKwYRz6",
        "outputId": "85c103c0-94e3-432d-8e73-352b8ad1b9de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up stopwords and lemmatizer\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "0yfujp1IYYyy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading documents from folder\n",
        "def load_documents(folder_path):\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "\n",
        "    print(f'Scanning folder: {folder_path}')\n",
        "    for filename in os.listdir(folder_path):\n",
        "        print(f'Found file: {filename}')\n",
        "        if filename.endswith(('.txt', '.doc', '.pdf')):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            content = ''\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "\n",
        "            if content:\n",
        "                data[doc_id] = content\n",
        "                doc_id_to_filename[doc_id] = filename\n",
        "                print(f'Loaded doc_id {doc_id} -> {filename}')\n",
        "                doc_id += 1\n",
        "\n",
        "    print(f'Total documents loaded: {len(data)}')\n",
        "    return data, doc_id_to_filename"
      ],
      "metadata": {
        "id": "2jNSFmdTYVNE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning text by removing unwanted characters and lemmatizing\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS and len(word) > 1]\n",
        "    return ' '.join(cleaned_tokens)"
      ],
      "metadata": {
        "id": "y0qUt2oIVFPq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building vector space model using TF-IDF\n",
        "def build_vector_space_model(data):\n",
        "    cleaned_docs = [clean_text(content) for content in data.values()]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n",
        "    return tfidf_matrix, vectorizer"
      ],
      "metadata": {
        "id": "L0-V5ZmyVFW5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating similarity scores between documents\n",
        "def calculate_similarity(tfidf_matrix, doc_id_to_filename):\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "    num_docs = len(doc_id_to_filename)\n",
        "    print('Similarity scores between documents: ')\n",
        "    for i in range(num_docs):\n",
        "        for j in range(i + 1, num_docs):\n",
        "            score = similarity_matrix[i][j]\n",
        "            doc1 = doc_id_to_filename[i]\n",
        "            doc2 = doc_id_to_filename[j]\n",
        "            print(f'Similarity between {doc1} and {doc2}: {score:.4f}')\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "q-zy3lywVFeM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    folder_path = './documents'\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    data, doc_id_to_filename = load_documents(folder_path)\n",
        "\n",
        "    # Print cleaned text preview\n",
        "    for doc_id, content in data.items():\n",
        "        cleaned = clean_text(content)\n",
        "        print(f'Doc {doc_id} cleaned text (first 100 chars): {cleaned[:100]}...')\n",
        "\n",
        "    # Build model\n",
        "    tfidf_matrix, vectorizer = build_vector_space_model(data)\n",
        "\n",
        "    # Calculate similarity (correct argument)\n",
        "    similarity_matrix = calculate_similarity(tfidf_matrix)\n",
        "\n",
        "    # Convert dictionary to list so index = doc_id\n",
        "    filenames = [doc_id_to_filename[i] for i in range(len(doc_id_to_filename))]\n",
        "\n",
        "    # Save results\n",
        "    with open('similarity_results.txt', 'w', encoding='utf-8') as result_file:\n",
        "        result_file.write('Similarity scores:\\n')\n",
        "\n",
        "        num_docs = len(filenames)\n",
        "        for i in range(num_docs):\n",
        "            for j in range(i + 1, num_docs):\n",
        "                score = similarity_matrix[i][j]\n",
        "                result_file.write(\n",
        "                    f'{filenames[i]} and {filenames[j]}: {score:.4f}\\n'\n",
        "                )\n",
        "\n",
        "    print('Results saved to similarity_results.txt')\n",
        "    print('Include snapshots of the results and code in your report.')\n",
        "    print('Upload the notebook to GitHub and share the link.')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Kp5RnFs-W5PX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}